{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tensor 3-Ex1 (tokenizer some sentense and sceama).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vbVmqUIEn8R3"},"source":["# 1. Simple tokenizer"]},{"cell_type":"code","metadata":{"id":"rA308r6MbB6N"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","sentences = [\n","    'I love my dog',\n","    'I love my cat',\n","    'You love my dog!',\n","    'Do you think my dog is amazing?'\n","]\n","\n","tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","\n","sequences = tokenizer.texts_to_sequences(sentences)\n","\n","padded = pad_sequences(sequences, maxlen=5)\n","print(\"\\nWord Index = \" , word_index)\n","print(\"\\nSequences = \" , sequences)\n","print(\"\\nPadded Sequences:\")\n","print(padded)\n","\n","\n","# Try with words that the tokenizer wasn't fit to\n","test_data = [\n","    'i really love my dog',\n","    'my dog loves my manatee'\n","]\n","\n","test_seq = tokenizer.texts_to_sequences(test_data)\n","print(\"\\nTest Sequence = \", test_seq)\n","\n","padded = pad_sequences(test_seq, maxlen=10)\n","print(\"\\nPadded Test Sequence: \")\n","print(padded)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Gv6oscJn8R9"},"source":["# 2. Tokernizer for sarcasm"]},{"cell_type":"code","metadata":{"id":"zkT3kgVZbHXQ"},"source":["!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \\\n","    -O /tmp/sarcasm.json\n","\n","import json\n","\n","with open(\"/tmp/sarcasm.json\", 'r') as f:\n","    datastore = json.load(f)\n","\n","\n","sentences = [] \n","labels = []\n","urls = []\n","for item in datastore:\n","    sentences.append(item['headline'])\n","    labels.append(item['is_sarcastic'])\n","    urls.append(item['article_link'])\n","\n","\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","tokenizer = Tokenizer(oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(sentences)\n","\n","word_index = tokenizer.word_index\n","print(len(word_index))\n","print(word_index)\n","sequences = tokenizer.texts_to_sequences(sentences)\n","padded = pad_sequences(sequences, padding='post')\n","print(padded[0])\n","print(padded.shape)"],"execution_count":null,"outputs":[]}]}